{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWwq6PBvjZh52kbD3ssWcL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mroshan454/Replicating-ViT-Research-Paper/blob/main/Modular_Functions_For_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modular Code Structure for ViT Replication\n",
        "\n",
        "This notebook explains the modular Python Scripts used in My Vision Transformer (ViT) Replication Project. Each Script is written to handle a specific part of the machine learning pipeline in a clear , reusable way - just like in the production codebases.\n"
      ],
      "metadata": {
        "id": "E9eLpoywI3zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 📦`data_setup.py` - Dataset and Dataloader Builder\n",
        "\n",
        "**Purpose** - This script helps in loading the image data into PyTorch `Dataloaders's`. It is written modularly so that it can be reused with any image classification dataset , simply by passing in the paths.\n",
        "\n"
      ],
      "metadata": {
        "id": "ybQVox53LA4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile data_setup.py\n",
        "\"\"\"\n",
        "This file contains functionality for creating PyTorch DataLoader's for\n",
        "image classification data\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from torchvision import datasets,transforms\n",
        "from torch.utils.data import dataloader\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(\n",
        "    train_data:str,\n",
        "    test_data:str,\n",
        "    transform:transforms.Compose,\n",
        "    batch_size:int,\n",
        "    num_workers:int=NUM_WORKERS\n",
        "    ):\n",
        "    \"\"\"Creates training and testing DataLoaders.\n",
        "    Takes in a training directory and testing directory path and turns\n",
        "    them into PyTorch Datasets and them into PyTorch Dataloader.\n",
        "\n",
        "    Args:\n",
        "    train_dir: Path to training directory.\n",
        "    test_dir: Path to testing directory.\n",
        "    transform: torchvision transforms to perform on training and testing data.\n",
        "    batch_size: Number of samples per batch in each of the Dataloaders.\n",
        "    num_workers: An integer for number of workers per DataLoader.\n",
        "\n",
        "    Returns:\n",
        "    A tuple of (train_dataloader,test_dataloader ,class_names).\n",
        "    Where class_names is a list of the target classes.\n",
        "    Example usage:\n",
        "    train_dataloader, test_dataloader, class_names = create_dataloaders(train_dir=path/to/train_dir,\n",
        "    test_dir = path/to/test_dir,\n",
        "    transform=some_transform,\n",
        "    batch_size=32,\n",
        "    num_workers=4)\n",
        "    \"\"\"\n",
        "    #Use ImageFolder to create dataset(s)\n",
        "    train_data = datasets.ImageFolder(train_dir,transform=transform)\n",
        "    test_data = datasets.ImageFolder(test_dir,transform=transform)\n",
        "    #Get Class Names\n",
        "    class_names = train_data.classes\n",
        "    #Turn images into DataLoaders\n",
        "    train_dataloader = DataLoader(train_data,\n",
        "                                  batch_size=batch_size,\n",
        "                                  shuffle=True,\n",
        "                                  num_workers=num_workers,\n",
        "                                  pin_memory=True)\n",
        "    test_dataloader = Dataloader(test_data,\n",
        "                                 batch_size=batch_size,\n",
        "                                 shuffle=False,\n",
        "                                 num_workers=num_workers,\n",
        "                                 pin_memory=True)\n",
        "    return train_dataloader , test_dataloader , class_names"
      ],
      "metadata": {
        "id": "s_Yj0d9ILAoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. ⚙️`engine.py` - Training and Evaluation Engine\n",
        "\n",
        "This script contains all training and evaluation logic for the model. It include modular functions for:\n",
        "* One-epoch training(`train_step`)\n",
        "* One-epoch testing(`test_step`)\n",
        "* Full training Pipeline over multiple epochs(`train`)"
      ],
      "metadata": {
        "id": "a4YmgH54K_-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/engine.py\n",
        "\"\"\"\n",
        "Contains functions for training and testing a PyTorch Model.\n",
        "\"\"\"\n",
        "from typing import Dict, List , Tuple\n",
        "\n",
        "import torch\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def train_step(model:torch.nn.Module,\n",
        "               dataloader:torch.utils.Dataloader,\n",
        "               loss_fn:torch.nn.Module,\n",
        "               optimizer:torch.optim.Optimizer,\n",
        "               device:torch.device) -> Tuple[float, float]:\n",
        "    \"\"\"Trains a PyTorch model for a single epoch.\n",
        "\n",
        "    Turns a target PyTorch model to training mode and then\n",
        "    runs through all of the required training steps (forward\n",
        "    pass, loss calculation, optimizer step).\n",
        "\n",
        "    Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    dataloader: A DataLoader instance for the model to be trained on.\n",
        "    loss_fn: A PyTorch loss function to minimize.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    A tuple of training loss and training accuracy metrics.\n",
        "    In the form (train_loss, train_accuracy). For example:\n",
        "\n",
        "    (0.1112, 0.8743)\n",
        "    \"\"\"\n",
        "    #Put the model in train mode\n",
        "    model_train()\n",
        "\n",
        "    #Setup train loss and training accuracy values\n",
        "    train_loss , train_acc = 0, 0\n",
        "\n",
        "    #Loop through the dataloader and batches\n",
        "    for batch,(X,y) in enumerate(dataloader):\n",
        "       #Send data to target device\n",
        "       X, y = X.to(device) , y.to(device)\n",
        "\n",
        "       #1. Forward Pass\n",
        "       y_pred = model(X)\n",
        "\n",
        "       #2. Calculate and accumulate the loss\n",
        "       loss = loss_fn(y_pred,y)\n",
        "       train_loss =+ loss.item()\n",
        "\n",
        "       #3. Optimizer Zero Grad()\n",
        "       optimizer.zero_grad()\n",
        "\n",
        "       #4. Loss Backward\n",
        "       loss.backward()\n",
        "\n",
        "       #5. Optimizer Step\n",
        "       optimizer.step()\n",
        "\n",
        "       #Calculate and accumulate accuaracy metric across all batches\n",
        "       y_pred_class = torch.argmax(torch.softmax(y_pred,dim=1),dim=1)\n",
        "       train_acc += (y_pred_class == y).sum().item()/len(pred)\n",
        "\n",
        "    #Adjust the metrics to get average loss and accuarcy per batch\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    train_acc = train_acc/ len(dataloader)\n",
        "\n",
        "    return train_loss , train_acc\n",
        "\n",
        "def test_step(model:torch.nn.Module,\n",
        "              dataloader:torch.utils.Dataloader,\n",
        "              loss_fn:torch.nn.Module,\n",
        "              optimizer:torch.optim.Optimizer,\n",
        "              device:torch.device) -> Tuple[float, float]:\n",
        "\n",
        "    \"\"\"Tests a PyTorch model for a single epoch.\n",
        "\n",
        "    Turns a target PyTorch model to \"eval\" mode and then performs\n",
        "    a forward pass on a testing dataset.\n",
        "\n",
        "    Args:\n",
        "    model: A PyTorch model to be tested.\n",
        "    dataloader: A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    A tuple of testing loss and testing accuracy metrics.\n",
        "    In the form (test_loss, test_accuracy). For example:\n",
        "\n",
        "    (0.0223, 0.8985)\n",
        "    \"\"\"\n",
        "    #Put the model in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    #Setup test loss and test accuracy values\n",
        "    test_loss , test_acc = 0,0\n",
        "\n",
        "    # Turn on inference context manager\n",
        "    with torch.inference_mode():\n",
        "      #Loop through data batches\n",
        "      for batch,(X,y) in enumerate(dataloader):\n",
        "        #Send data to target device\n",
        "        X , y = X.to(device), y.to(device)\n",
        "\n",
        "        #1. Forward Pass\n",
        "        test_pred_logits = model(X)\n",
        "\n",
        "        #2. Calculate the loss\n",
        "        loss = loss_fn(test_pred_logits,y)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        #Calculate and accumulate the accuracy\n",
        "        test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "        test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "    #Adjust the metrics to get average loss and accuracy per batch\n",
        "    test_loss = test_loss/len(dataloader)\n",
        "    test_acc = test_acc/len(dataloader)\n",
        "    return test_loss , test_acc\n",
        "\n",
        "def train(model:torch.nn.Module,\n",
        "          train_dataloader:torch.utils.data.Dataloader,\n",
        "          test_dataloader:torch.utils.data.Dataloader,\n",
        "          optimizer:torch.optim.Optimizer,\n",
        "          loss_fn:torch.nn.Module,\n",
        "          epochs:int,\n",
        "          device:torch.device) -> Dict[str,List[float]]:\n",
        "\n",
        "    \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "    Passes a target PyTorch models through train_step() and test_step()\n",
        "    functions for a number of epochs, training and testing the model\n",
        "    in the same epoch loop.\n",
        "\n",
        "    Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "    Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for\n",
        "    each epoch.\n",
        "    In the form: {train_loss: [...],\n",
        "              train_acc: [...],\n",
        "              test_loss: [...],\n",
        "              test_acc: [...]}\n",
        "    For example if training for epochs=2:\n",
        "             {train_loss: [2.0616, 1.0537],\n",
        "              train_acc: [0.3945, 0.3945],\n",
        "              test_loss: [1.2641, 1.5706],\n",
        "              test_acc: [0.3400, 0.2973]}\n",
        "    \"\"\"\n",
        "\n",
        "    # Loop through training and testing steps for a number of epochs\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc=train_step()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "3ACPSYUOO-4T",
        "outputId": "b0375614-d809-451a-dec5-03db8e681046"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/engine.py\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'going_modular/engine.py'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-753176809.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'writefile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'going_modular/engine.py'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\"\"\"\\nContains functions for training and testing a PyTorch Model.\\n\"\"\" \\nfrom typing import Dict, List , Tuple \\n\\nimport torch \\n\\nfrom tqdm.auto import tqdm\\n\\ndef train_step(model:torch.nn.Module,\\n               dataloader:torch.utils.Dataloader,\\n               loss_fn:torch.nn.Module,\\n               optimizer:torch.optim.Optimizer,\\n               device:torch.device) -> Tuple[float, float]:\\n    \"\"\"Trains a PyTorch model for a single epoch.\\n\\n    Turns a target PyTorch model to training mode and then\\n    runs through all of the required training steps (forward\\n    pass, loss calculation, optimizer step).\\n\\n    Args:\\n    model: A PyTorch model to be trained.\\n    dataloader: A DataLoader instance for the model to be trained on.\\n    loss_fn: A PyTorch loss function to minimize.\\n    optimizer: A PyTorch optimizer to help minimize the loss function.\\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\\n\\n    Returns:\\n    A tuple of training loss and training accuracy metrics.\\n    In the form (train_loss, train_accuracy). For example:\\n\\n    (0.1112, 0.8743)\\n    \"\"\"\\n    #Put the model in train mode \\n    model_train()\\n\\n    #Setup train loss and training accuracy values \\n    train_loss , train_acc = 0, 0 \\n\\n    #Loop through the dataloader and...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m       \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2471\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-98>\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/magics/osm.py\u001b[0m in \u001b[0;36mwritefile\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'going_modular/engine.py'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3q4RZbpfZW2R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}